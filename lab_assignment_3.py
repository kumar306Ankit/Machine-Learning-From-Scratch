# -*- coding: utf-8 -*-
"""Lab_Assignment_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aSoiPVRY4RhJN9Y_4XxWqL45EgfWvSIr

# Question 1

## Part 1
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv('/content/titanic.csv')
dataset

# drop unwanted columns from dataset
dataset = dataset.drop(["PassengerId","Name","Ticket","Cabin"],axis=1)
dataset

# Split dataset into dependent and independent variable
X = pd.DataFrame(dataset.iloc[:, :-1].values)
y = pd.DataFrame(dataset.iloc[:, -1].values)

print(X)

print(y)

# Finding out how much null values present in each column
X.isnull().sum()

y.isnull().sum()

# by using simpleimputer we replace all null values by it columnn mean in numeric column and in categorical datatype column we replace null value by most frequent one.
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X.iloc[:, 2:3])
X.iloc[:, 2:3] = imputer.transform(X.iloc[:, 2:3])

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit(X.iloc[:, 4:6])
X.iloc[:, 4:6] = imputer.transform(X.iloc[:, 4:5])

X.isnull().sum()

y.isnull().sum()

# Encode the data using labelencoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X.iloc[:,1] = le.fit_transform(X.iloc[:,1])
print(X)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X.iloc[:,4] = le.fit_transform(X.iloc[:,4])
print(X)

# Vizualise Data
import plotly.express as px
df = px.data.tips()
fig = px.bar(dataset, x="Sex", y="Survived", barmode="group")
fig.show()

# Vizualise Data
import plotly.express as px
df = px.data.tips()
fig = px.bar(dataset, x="Sex", y="Survived", color="Age", barmode="group")
fig.show()

# Vizualise Data
import plotly.express as px
df = px.data.tips()
fig = px.bar(dataset, x="Embarked", y="Survived", barmode="group")
fig.show()

# Vizualise Data
import plotly.express as px
df = px.data.tips()
fig = px.bar(dataset, x="Embarked", y="Survived", color="Age", barmode="group")
fig.show()

#train and split dataset into 80% train and 20% test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

print(X_train)

print(X_test)

print(y_train)

print(y_test)

# apply featurescaling on column 6 and 7 to make data comparable for models
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train.iloc[:,2:4] = sc.fit_transform(X_train.iloc[:,2:4])
X_test.iloc[:,2:4] = sc.transform(X_test.iloc[:,2:4])

print(X_train)

print(X_test)

"""## Part 2

This is a Gaussian Variant of Naive Bayes because features take up a continuous value and are not discrete, so we assume that these values are sampled from a gaussian distribution.
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

"""## Part 3"""

model = GaussianNB()
model.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

y_pred = model.predict_proba(X_test)[:,1]

from sklearn.metrics import roc_auc_score
roc_auc = roc_auc_score(y_test, y_pred)
print("ROC AUC:", roc_auc)

from sklearn.metrics import accuracy_score, roc_curve, auc
import matplotlib.pyplot as plt
Fpr, Tpr, Thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(Fpr, Tpr)

plt.plot(Fpr, Tpr, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--')  # random guess line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.show()

"""## Part 4"""

from sklearn.model_selection import KFold

from sklearn.model_selection import KFold
n = 5
MODEL2 = KFold(n_splits=n, shuffle=True)
roc = []
for train_index, test_index in MODEL2.split(X):
    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]
    y_train, y_test = y.iloc[train_index,:], y.iloc[test_index,:]
    model1 = model
    model1.fit(X_train, y_train)
    y_pred1 = model1.predict(X_test)
    roc.append(roc_auc_score(y_test, y_pred1))
# Calculate the mean ROC AUC score across all iterations
mean_roc_auc = np.mean(roc)
print("Mean ROC AUC:", mean_roc_auc)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

P = model.predict_proba(X_test)
print(P)

"""## Part 5"""

X

import seaborn as sns
sns.kdeplot(X[2],X[3],hue=y,data=X)

"""## Part 6"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""# Question 2"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset1 = pd.read_csv('/content/dataset (1).csv')
dataset1

"""## Part 1"""

import matplotlib.pyplot as plt
import pandas as pd

for i in dataset1.columns:
  column = dataset1[i]
  plt.hist(column,edgecolor='black',bins = 20)
  plt.title("Histogram of Column Values")
  plt.xlabel(i)
  plt.ylabel("Frequency")
  plt.show()

"""## Part 2"""

class_1 = 0
class_2 = 0
class_3 = 0
l = [i for i in dataset1['Y']]
Length = len(l)
for i in range(Length):
  if l[i] == 1:
    class_1 +=1
  elif l[i] == 2:
    class_2 += 1
  else:
    class_3 += 1
prior_probability_1 = class_1/Length
prior_probability_2 = class_2/Length
prior_probability_3 = class_3/Length
prior_probability = [prior_probability_1,prior_probability_2,prior_probability_3]
print("The prior probability of class {} is {}".format(1, prior_probability_1))
print("The prior probability of class {} is {}".format(2, prior_probability_2))
print("The prior probability of class {} is {}".format(3, prior_probability_3))

"""## Part 3"""

def discretize_column(column, n_bins):
    listcolumn=list(column)
    maxnum=max(listcolumn)
    minnum=min(listcolumn)
    bin_width=(maxnum+0.01-minnum)/n_bins
    bins=[]
    for i in range(1,n_bins+1):
        bins.append(minnum+i*bin_width)
    result = []
    for value in listcolumn:
        for i in range(0,n_bins):
            if value < bins[i]:
                result.append(i)
                break
    return result
allcolumnbins=[]
for column in ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6']:
    allcolumnbins.append(discretize_column(dataset1[column], 15))

discdf=pd.DataFrame(np.array(allcolumnbins).T)
discdf[7]=dataset1['Y']
discdf

"""## Part 4"""

for i in range(1,4):
    print("Class=", i)
    g = discdf[discdf[7] == i]
    d1={}
    for column in range(0,7):
        d=g[column].value_counts()
        op = {}
        for j in d.index:
            p = d[j]/len(g)
            op[j] = p
        d1[column] = op
    print(d1)

"""## Part 5"""

for i in range(1,4):
    print("Class=", i)
    g = discdf[discdf[7] == i]
    d1={}
    for column in range(0,7):
        d=g[column].value_counts()
        d.plot(kind="bar")
        plt.show()

"""## Part 6"""

import numpy as np
likelihood = []
unnormalized_posteriors = [prior_probability[i] * likelihood[i] for i in range(len(prior_probability))]
posteriors = unnormalized_posteriors / sum(unnormalized_posteriors)