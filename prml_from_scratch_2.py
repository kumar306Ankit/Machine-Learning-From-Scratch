# -*- coding: utf-8 -*-
"""PRML_From_Scratch_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15F4nUgbg83lnZLoJNIpzyDVzulRcl8R8

# **Question 1**

This question involves performing regression using a decision tree. You are permitted to use the sklearn library for this question. The dataset involves energy analysis, and this problem is centered around understanding how machine learning is applied in the industry. The dataset contains eight attributes (X1, …, X8) (representative of different properties of buildings like height, roof area, etc.) and one response (Y1) (the heating load for the building). The aim is to use the eight features to predict Y1.

##1.2
Preprocess the data. Split it using a 70:10:20 ratio, which represents training:validation:testing
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

df= pd.read_csv('/content/ENB2012_data.xlsx - Φύλλο1.csv')

df

#handling null values
imputer = SimpleImputer(missing_values=np.nan, strategy="mean")
imputer.fit(df)
df=pd.DataFrame(imputer.transform(df))
df

#Splitting into train, validation and test
X=df.iloc[:, 0:8]
y=df.iloc[:,8]
X_train, X_rem, y_train, y_rem = train_test_split(X,y,train_size=0.7)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem,train_size=2/3)

#Scaling the data
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_valid=scaler.fit_transform(X_valid)
X_test=scaler.fit_transform(X_test)#X_train,test,valid is array

"""##1.2
Write a function to train the data using a regression decision tree. The function varies hyper-parameters to find the tree that generalizes best (based on its performance on the validation set). So, you need to train on the 70% training data and check performance on the 10% validation data. Properly explain the thought process behind which hyper-parameters you vary and the expected effects in the report. Make plots of validation MSE to support your arguments. [10 marks] Note: Here we are evaluating the method involved in the generalization rather than the achieved accuracy. Please refrain from using any inbuilt scikit library functions for hyper-parameterization (eg:- grid search)
"""

def evaluate(X_train, y_train, X_valid, y_valid, maxdepth, maxleafnodes):
  regressor = DecisionTreeRegressor(random_state = 0,max_depth=maxdepth, max_leaf_nodes=maxleafnodes)
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_valid)
  np.set_printoptions(precision=2)
  return mean_squared_error(y_valid, y_pred)

score = []
x = np.arange(1,11,1)
for i in x:
  score.append(evaluate(X_train, y_train, X_valid, y_valid ,i, None))
plt.plot(x,score)
plt.xlabel('Max_depth')
plt.ylabel('MSE')
plt.show()
print("For max_depth {}, error is minimum, that is {}".format(x[9], min(score)))

score = []
x = np.arange(2,50,1)
for i in x:
  score.append(evaluate(X_train, y_train, X_valid, y_valid ,None, i))
plt.plot(x,score)
plt.xlabel('max_leaf_nodes')
plt.ylabel('MSE')
plt.show()
print("For max_leaf_nodes {}, error is minimum, that is {}".format(x[47], min(score)))

"""## 1.3

Perform Hold-out cross validation, 5-fold cross-validation and repeated-5-fold validation using the optimal hyper-parameters decided in the previous question. Finally, calculate the mean squared error between the predicted and the ground-truth values in the test data for your best model. Also, plot the decision tree created
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

df= pd.read_csv('/content/ENB2012_data.xlsx - Φύλλο1.csv')

df

class preprocess:
  def __init__(self,data):
    self.data = data
  def nullvalue(self):
    from sklearn.impute import SimpleImputer
    for i in range(len(self.data.columns)): 
        if self.data.iloc[:,i].dtype==object:
            imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
            imputer.fit(self.data.iloc[:,i:i+1])
            self.data.iloc[:,i:i+1] = imputer.transform(self.data.iloc[:,i:i+1])
        else:
          imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
          imputer.fit(self.data.iloc[:,i:i+1])
          self.data.iloc[:,i:i+1] = imputer.transform(self.data.iloc[:,i:i+1])
  def encoding(self):
    from sklearn.preprocessing import LabelEncoder
    for i in self.data.columns: 
        if self.data[i].dtype==object:
            le = LabelEncoder()
            self.data[i] = le.fit_transform(self.data[i])
  def split(self,train,test):
    from sklearn.model_selection import train_test_split
    self.train = train
    self.X = self.data.iloc[:,:-1]
    self.y = self.data.iloc[:,-1]
    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, random_state=42, train_size = self.train)
    return self.X,self.y,self.X_train, self.X_test, self.y_train, self.y_test
  def normalization(self,X1,X2):
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    X1= sc.fit_transform(X1)
    X2 = sc.transform(X2)
    return X1,X2

D = preprocess(df)
D.nullvalue()
D.encoding()
X1,y1,X_train, X_test, y_train, y_test = D.split(0.8,0.2)
X_train,X_test = D.normalization(X_train,X_test)

"""Hold-out cross validation"""

def evaluate(X_train, y_train, X_valid, y_valid, maxdepth, maxleafnodes):
  regressor = DecisionTreeRegressor(random_state = 0,max_depth=maxdepth, max_leaf_nodes=maxleafnodes)
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_valid)
  np.set_printoptions(precision=2)
  return mean_squared_error(y_valid, y_pred)

evaluate(X_train, y_train, X_test, y_test, 10, 49)

"""5-Fold Cross-Validation"""

from sklearn.model_selection import KFold
n = 5
k=1
MODEL2 = KFold(n_splits=n, shuffle=True)
accuracies = []
y=pd.DataFrame(y)
l = []
for train_index, test_index in MODEL2.split(X):
    X_train, X_test = np.array(X.iloc[train_index,:]), np.array(X.iloc[test_index,:])
    y_train, y_test = np.array(y.iloc[train_index,:]), np.array(y.iloc[test_index,:])
    mean_square_error = evaluate(X_train, y_train, X_test, y_test, 10, 49)
    l.append(mean_square_error)
    print(mean_square_error)
print("Average MSE for 5fold Cross Validation:",np.mean(l))

"""Repeated cross validation"""

from sklearn.model_selection import RepeatedKFold
cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)

l1 = []
for train_index, test_index in cv.split(x): 
  X_train, X_test = np.array(X.iloc[train_index,:]), np.array(X.iloc[test_index,:])
  y_train, y_test = np.array(y.iloc[train_index,:]), np.array(y.iloc[test_index,:])
  mean_square_error = evaluate(X_train, y_train, X_test, y_test, 10, 49)
  l1.append(mean_square_error)
print("Average MSE for Repeated Cross Validation:",np.mean(l1))

"""plot the decision tree created"""

def DecisionPlot(X_train, y_train, X_valid, y_valid, maxdepth, maxleafnodes):
  from sklearn.tree import export_graphviz 
  import matplotlib.pyplot as plt
  from sklearn import tree
  regressor = DecisionTreeRegressor(random_state = 0,max_depth=maxdepth, max_leaf_nodes=maxleafnodes)
  regressor.fit(X_train, y_train)
  plt.figure(figsize=(100,100))
  tree.plot_tree(regressor)
DecisionPlot(X_train, y_train, X_test, y_test, 10, 49)

"""## 1.4
Use L1 and L2 as two different criterion for split and plot the decision boundary you obtain. Which loss works better?Explain why one of them performed better on the given dataset

L1
"""

L1_model_train = DecisionTreeRegressor(splitter = 'best',criterion = 'absolute_error')
L1_model_train.fit(X_train,y_train)
y_pred = L1_model_train.predict(X_test)
print('MSE',mean_squared_error(y_test, y_pred))

"""L2"""

L2_model_train = DecisionTreeRegressor(splitter = 'best',criterion= 'squared_error')  
L2_model_train.fit(X_train,y_train)  
y_pred = L2_model_train.predict(X_test)
print('MSE',mean_squared_error(y_test, y_pred))

"""# **Question 2**

## Classification
"""

class preprocess:
  def __init__(self,data):
    self.data = data
  def nullvalue(self):
    from sklearn.impute import SimpleImputer
    for i in range(len(self.data.columns)): 
        if self.data.iloc[:,i].dtype==object:
            imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
            imputer.fit(self.data.iloc[:,i:i+1])
            self.data.iloc[:,i:i+1] = imputer.transform(self.data.iloc[:,i:i+1])
        else:
          imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
          imputer.fit(self.data.iloc[:,i:i+1])
          self.data.iloc[:,i:i+1] = imputer.transform(self.data.iloc[:,i:i+1])
  def encoding(self):
    from sklearn.preprocessing import LabelEncoder
    for i in self.data.columns: 
        if self.data[i].dtype==object:
            le = LabelEncoder()
            self.data[i] = le.fit_transform(self.data[i])
  def split(self,train,test):
    from sklearn.model_selection import train_test_split
    self.train = train
    self.X = self.data.iloc[:,:-1]
    self.y = self.data.iloc[:,-1]
    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, random_state=42, train_size = self.train)
    return self.X,self.y,self.X_train, self.X_test, self.y_train, self.y_test
  def normalization(self,X1,X2):
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    X1= sc.fit_transform(X1)
    X2 = sc.transform(X2)
    return X1,X2

"""## 2.1
Classification [ 50 marks] For this task we will use only Petal length and Petal width attributes from the Iris dataset. Split the dataset into training and test in the ratio 80:20.
 1. Train a Decision Tree Classifier (max depth=2) on the pre-processed dataset. Plot the decision boundaries of the tree as well as indicate the depth at which each split was made.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
col_names = [0,1,2,3,4]
df1 = pd.read_csv('/content/iris.data',names = col_names)
df1 = pd.DataFrame(df1)
df1 = df1.drop(df1.columns[[0, 1]], axis=1)

D = preprocess(df1)
D.nullvalue()
D.encoding()
X1,y1,X_train2, X_test2, y_train2, y_test2 = D.split(0.8,0.2)
X_train2,X_test2 = D.normalization(X_train2,X_test2)

def evaluate(X_train, y_train, X_valid, y_valid,maxdepth):
  from sklearn.tree import DecisionTreeClassifier
  regressor = DecisionTreeClassifier(random_state = 0,max_depth=maxdepth)
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_valid)
  np.set_printoptions(precision=2)
  return mean_squared_error(y_valid, y_pred)

evaluate(X_train2, y_train2, X_train2, y_train2,2)

"""Decsion Boundary"""

print(y_train)
def Boundary(clf,X,y):
    X2, y2 = np.meshgrid(np.linspace(np.min(X[:,0]), np.max(X[:,0]), 60), np.linspace(np.min(X[:,1]), np.max(X[:,1]), 60))
    print(y.shape)
    clf.fit(X[:,0:2],y)
    Z = clf.predict(pd.DataFrame(np.c_[X2.ravel(), y2.ravel()],columns = [0,1]))
    Z = Z.reshape(X2.shape)
    plt.contourf(X2, y2, Z,alpha=0.1)
    x=pd.DataFrame(X)
    sns.scatterplot(data=x,x = 0,y = 1,hue = y)
    return plt.show()

from sklearn.tree import DecisionTreeClassifier
regressor2 = DecisionTreeClassifier(random_state = 0,max_depth=2)
regressor2.fit(X_train2, y_train2)

Boundary(regressor2,X_train2,y_train2)

"""## 2.2
Remove the widest Iris-Versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree. Plot the Decision boundary for this case.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
col_names = [0,1,2,3,4]
df2 = pd.read_csv('/content/iris.data',names = col_names)
df2 = pd.DataFrame(df2)
df2 = df2.drop(df2.columns[[0, 1]], axis=1)

df2 = df2[(df2[2] != 4.8) & (df2[3] != 1.8)]

Z = preprocess(df2)
Z.nullvalue()
Z.encoding()
X2,y2,X_train3, X_test3, y_train3, y_test3 = Z.split(0.8,0.2)
X_train3,X_test3 = D.normalization(X_train3,X_test3)

evaluate(X_train3, y_train3, X_train3, y_train3,2)

"""Decsion Boundary"""

from sklearn.tree import DecisionTreeClassifier
regressor3 = DecisionTreeClassifier(random_state = 0,max_depth=2)
regressor3.fit(X_train3, y_train3)

Boundary(regressor3,X_train3, y_train3)

"""## 2.3
Train a Decision Tree Classifier with (max-depth = None) on the pre-processed dataset. Plot the Decision boundary for the same. Compare and analyse the results with those in part 1.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
col_names = [0,1,2,3,4]
df3 = pd.read_csv('/content/iris.data',names = col_names)
df3 = pd.DataFrame(df3)
df3 = df3.drop(df3.columns[[0, 1]], axis=1)

D1 = preprocess(df3)
D1.nullvalue()
D1.encoding()
X4,y4,X_train4, X_test4, y_train4, y_test4 = D1.split(0.8,0.2)
X_train4,X_test4 = D.normalization(X_train4,X_test4)

evaluate(X_train2, y_train2, X_train2, y_train2,None)

"""Decsion Boundary"""

from sklearn.tree import DecisionTreeClassifier
regressor4 = DecisionTreeClassifier(random_state = 0,max_depth=None)
regressor4.fit(X_train4, y_train4)

Boundary(regressor4,X_train4,y_train4)

"""## 2.4
Create a random dataset having 2 attributes(X1 and X2), and 2 classes (y=0 and y=1) .X1,X2 are randomly sampled from the range (0,5). y=0 when X1<2.5, and y=1 when X1>2.5. The dataset should have 100 data points for both the classes. Train a decision tree for such a dataset(max-depth=2). Plot the obtained decision boundaries.Now, rotate the datapoints by 45 degrees in clockwise direction about the origin (X1=0,X2=0). Train another decision tree classifier using sklearn. Compare the plots obtained in both the above methods.[15 marks]
"""

dataset1 = np.random.uniform(0,2.5, size=(100, 2))# min value 0 max value 2.5
dataset2 = np.random.uniform(2.5,5, size=(100, 2))# min value 2.5 max value 5

dataset = np.concatenate((dataset1,dataset2),axis=0)

column1 = dataset1[:,0]
column2 = dataset[:,1]
classes = np.zeros(200)
a = 99
while a !=0:
  if column1[a] > 2.5:
    classes[a] = 1 
  else:
    classes[a] = 1
  a-=1

classes= classes.reshape(-1,1)

data = np.concatenate((dataset,classes),axis=1)

data = pd.DataFrame(data)

D1 = preprocess(data)
D1.nullvalue()
X5,y5,X_train5, X_test5, y_train5, y_test5 = D1.split(0.8,0.2)

evaluate(X_train5, y_train5, X_train5, y_train5,2)

"""Decsion Boundary"""

from sklearn.tree import DecisionTreeClassifier
regressor5 = DecisionTreeClassifier(random_state = 0,max_depth=2)
regressor5.fit(X_train5, y_train5)

Boundary(regressor5,np.array(X_train5),np.array(y_train5))

"""## **Regression**

## 2.6
Train two decision tree models, one with max_depth = 2 and another with max_depth = 3. Plot the regression predictions at each depth for each max_depth( for e.g., at depths 0,1 for max_depth = 2) using a line plot. Next, make a scatter plot of the data points on the same plots
"""

Ta = pd.read_csv('/content/task.csv')

T1 = preprocess(Ta)
T1.nullvalue()
X6,y6,X_train6, X_test6, y_train6, y_test6 = T1.split(0.8,0.2)

def evaluate(X_train, y_train, X_valid, y_valid, maxdepth):
  from sklearn.tree import DecisionTreeRegressor
  from sklearn.metrics import mean_squared_error
  regressor = DecisionTreeRegressor(max_depth=maxdepth)
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_valid)
  np.set_printoptions(precision=2)
  return mean_squared_error(y_valid, y_pred)

def BoundaryRegressor(clf,X,y):
    X2 = np.linspace(np.min(X[0]), np.max(X[0]), 60)
    clf.fit(X[[0]],y)
    Z = clf.predict(X2.reshape(-1,1))
    plt.plot(X2,Z,color = 'orange')
    sns.scatterplot(data=X,x = 0,y = y,hue = y)
    return plt.show()

"""maxdepth 2"""

from sklearn.tree import DecisionTreeRegressor
Tregressor = DecisionTreeRegressor(max_depth=2)
Tregressor.fit(X_train6, y_train6)

X_train6
X_train6.columns=[0]

BoundaryRegressor(Tregressor,X_train6,y_train6)

"""maxDepth 0"""

from sklearn.tree import DecisionTreeRegressor
Tregressor = DecisionTreeRegressor(max_depth=0.01) #max depth  0  is not possibe
Tregressor.fit(X_train6, y_train6)

X_train6
X_train6.columns=[0]

BoundaryRegressor(Tregressor,X_train6,y_train6)

"""maxDepth 1"""

from sklearn.tree import DecisionTreeRegressor
Tregressor = DecisionTreeRegressor(max_depth=1)
Tregressor.fit(X_train6, y_train6)

X_train6
X_train6.columns=[0]

BoundaryRegressor(Tregressor,X_train6,y_train6)

"""maxDepth 3"""

from sklearn.tree import DecisionTreeRegressor
Tregressor = DecisionTreeRegressor(max_depth=3)
Tregressor.fit(X_train6, y_train6)

X_train6
X_train6.columns=[0]

BoundaryRegressor(Tregressor,X_train6,y_train6)

"""## 2.7
Plot the data points and make a line graph to show the decision tree fits on the dataset in two cases: min_samples_leaf = 0 and min_samples_leaf = 10. Analyze the two plots and explain your findings in the report.

min_samples_leaf=0
"""

from sklearn.tree import DecisionTreeRegressor
Tregressor = DecisionTreeRegressor(min_samples_leaf=0.0001)
Tregressor.fit(X_train6, y_train6)

X_train6
X_train6.columns=[0]

BoundaryRegressor(Tregressor,X_train6,y_train6)

"""min_samples_leaf=10"""

from sklearn.tree import DecisionTreeRegressor
Tregressor = DecisionTreeRegressor(min_samples_leaf=10)
Tregressor.fit(X_train6, y_train6)

X_train6
X_train6.columns=[0]

BoundaryRegressor(Tregressor,X_train6,y_train6)

"""# **Question 3**

## 3.1
"""

pip install palmerpenguins

import pandas as pd
from palmerpenguins import load_penguins

df=load_penguins()

df['year'] = df['year'].replace(2007, "2007")     #converting numeric values of year to object
df['year'] = df['year'].replace(2008, "2008")
df['year'] = df['year'].replace(2009, "2009")
df

import numpy as np
import pandas as pd

"""Preprocessing"""

class preprocess:
  def __init__(self,data):
    self.data = data

  def nullvalue(self):
    from sklearn.impute import SimpleImputer
    for i in range(len(self.data.columns)): 
        if self.data.iloc[:,i].dtype==object:
            imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
            imputer.fit(self.data.iloc[:,i:i+1])
            self.data.iloc[:,i:i+1] = imputer.transform(self.data.iloc[:,i:i+1])
        else:
          imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
          imputer.fit(self.data.iloc[:,i:i+1])
          self.data.iloc[:,i:i+1] = imputer.transform(self.data.iloc[:,i:i+1])

  def encoding(self):
    from sklearn.preprocessing import LabelEncoder
    for i in self.data.columns: 
        if self.data[i].dtype==object:
            le = LabelEncoder()
            self.data[i] = le.fit_transform(self.data[i])
        
        

  def split(self,train,test):
    from sklearn.model_selection import train_test_split
    self.train = train
    self.X = self.data.iloc[:,1:8]
    self.y = self.data.iloc[:,0]
    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, random_state=42, train_size = self.train)
    return self.X,self.y,self.X_train, self.X_test, self.y_train, self.y_test

  def normalization(self,X1,X2):
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    X1= sc.fit_transform(X1)
    X2 = sc.transform(X2)
    return X1,X2

D = preprocess(df)
D.nullvalue()
D.encoding()      
X,y,X_train, X_test, y_train, y_test = D.split(0.7,0.3)       #splitting into train and test
X

"""Visualization"""

import matplotlib.pyplot as plt
for i in df.columns:
  column = df[i]
  plt.hist(column,edgecolor='black',bins = 20)
  plt.title("Histogram of Column Values")
  plt.xlabel(i)
  plt.ylabel("Frequency")
  plt.show()

"""## Method 2 Vizualize"""

import plotly.express as px
fig = px.bar(df, x="bill_length_mm", y="species", barmode="group")
fig.show()

import plotly.express as px
fig = px.bar(df, x="sex", y="species", barmode="group")
fig.show()

import plotly.express as px
fig = px.bar(df, x="island", y="species", barmode="group")
fig.show()

import plotly.express as px
fig = px.bar(df, x="bill_depth_mm", y="species", barmode="group")
fig.show()

import plotly.express as px
fig = px.bar(df, x="body_mass_g", y="species", barmode="group")
fig.show()

import plotly.express as px
fig = px.bar(df, x="flipper_length_mm", y="species", barmode="group")
fig.show()

"""## 3.2
Implement the cost function as per your roll num. B21EE007 is odd so 
Odd roll numbers - Gini index

"""

def gini_func(dataset):   #this function will find the gini impurity of that dataset
  unique_classes, class_counts = np.unique(dataset, return_counts=True)     # give array of unique classes and array of counts of each class
  n=len(dataset)
  prob = class_counts/n
  p_s=0
  for i in range(len(unique_classes)):
    p_s+=prob[i]**2
  gini_cost=1-p_s       
  return(gini_cost)
p1=gini_func(y)     #example
print(p1)

def gini(X, y, feature_name, split_value):
    X_split_left_side = X[X[feature_name] <= split_value][feature_name]
    y_split_left_side = y[X[feature_name] <= split_value]
    X_split_right_side = X[X[feature_name] > split_value][feature_name]
    y_split_right_side = y[X[feature_name] > split_value]
    l1 = len(X_split_left_side)
    l2 = len(X_split_right_side)
    giniProbability = (l1/len(X))*gini_func(y_split_left_side) + (l2/len(X))*gini_func(y_split_right_side)
    return giniProbability
print(gini(X,y,'island',0))

"""## 3.3
In order for the decision tree to work successfully, continuous variables need to be converted to categorical variables first. To do this, you need to implement a decision function that makes this split. Let us call that cont_to_cat(). The details of the function are the following. [10 marks]:-

a. Assume that the continuous variables are independent of each other   i.e. assuming 2 continuous variables A and B, the split of A does not in any way affect the split you will perform in B. 

b. The continuous variables should only be split into 2 categories, and the optimal split is one that divides the samples the best, based on the value of the function you have been allotted (as per your roll number).
"""

X['bill_length_mm'].dtype

def cont_to_cat(X, y):
    continous_data = ['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm', 'body_mass_g']
    for column_name in continous_data:
        n = (max(X[column_name]) - min(X[column_name])) / 1500
        splt = np.arange(min(X[column_name]), max(X[column_name]), n)
        split_values = [gini(X, y, column_name, i) for i in splt]
        split_values_min_gini = splt[np.argmin(split_values)]
        X[column_name] = np.where(X[column_name] <= split_values_min_gini, 0, 1)
    return X
X = cont_to_cat(X, y)

"""## 3.4
After step 2, all the attributes would have categorical values, so now you can go ahead and implement the training function. This would include implementing the following helper functions: [25 marks]

a. Get the attribute that leads to the best split
"""

def bestSplit(X, y):
    least_gini_impurity = 1.0
    best_feature_name = None
    Split_Value = 0
    for i in X.columns:
      unique_values_in_feature = []
      for j in X[i]:
        if j not in unique_values_in_feature:
          unique_values_in_feature.append(j)
      for value in unique_values_in_feature:
        score = gini(X, y,i,value)
        if score < least_gini_impurity:
            least_gini_impurity = score
            Split_Value = value
            best_feature_name = i
            #print(value)
            #print(least_gini_impurity)
            #print(best_feature_name)
    return best_feature_name,Split_Value
best_feature_name,Split_Value = bestSplit(X, y)

"""b. Make that split """

Xleft_child = X[X[best_feature_name] <= Split_Value]
Xright_child = X[X[best_feature_name] > Split_Value]
yleft_child = y[X[best_feature_name] <= Split_Value]
yright_child = y[X[best_feature_name] > Split_Value]

left_child

right_child

"""c. Repeat these steps for the newly-created split"""

def check_child_Leaf_Node(y):
  if len(set(y)) <= 1:
    return True
  else: 
    return False

class node:
  def __init__(self, parent,feature,X,y, depth):
    self.feature = feature
    self.X = X
    self.y = y
    self.pVal = None
    self.children = []
    self.parent = parent
    self.depth = depth



class DecisionTree:
  def __init__(self, X, y, max_depth):
    self.X = X
    self.y = y
    self.max_depth = max_depth
  def gini_func(self, dataset):  
    unique_classes, class_counts = np.unique(dataset, return_counts=True)    
    n=len(dataset)
    prob = class_counts/n
    p_s=0
    for i in range(len(unique_classes)):
      p_s+=prob[i]**2
    gini_cost=1-p_s       
    return(gini_cost)
  def gini(self, X, y, feature_name, split_value):
    X_split_left_side = X[X[feature_name] <= split_value][feature_name]
    y_split_left_side = y[X[feature_name] <= split_value]
    X_split_right_side = X[X[feature_name] > split_value][feature_name]
    y_split_right_side = y[X[feature_name] > split_value]
    l1 = len(X_split_left_side)
    l2 = len(X_split_right_side)
    giniProbability = (l1/len(X))*self.gini_func(y_split_left_side) + (l2/len(X))*self.gini_func(y_split_right_side)
    return giniProbability

  def bestSplit(self, X, y):
    least_gini_impurity = 1.0
    best_feature_name = 'flipper_length_mm'
    Split_Value = 0
    for i in X.columns:
      unique_values_in_feature = []
      for j in X[i]:
        if j not in unique_values_in_feature:
          unique_values_in_feature.append(j)
      for value in unique_values_in_feature:
        score = gini(X, y,i,value)
        if score <= least_gini_impurity:
            least_gini_impurity = score
            Split_Value = value
            best_feature_name = i
            #print(value)
            #print(least_gini_impurity)
            #print(best_feature_name)
    return best_feature_name,Split_Value

  def leaf_node(self, node_):
    return self.gini_func(node_.y) == 0 or check_child_Leaf_Node(node_.y) or node_.depth > self.max_depth

  def dfs(self, node_):

    if (self.leaf_node(node_)):
      return node_

    node_.feature, splt_val = self.bestSplit(node_.X, node_.y)
    print(node_.feature)
    node_.pVal = splt_val
    Xleft_child = node_.X[node_.X[node_.feature] <= splt_val]
    Xright_child = node_.X[node_.X[node_.feature] > splt_val]
    yleft_child = node_.y[node_.X[node_.feature] <= splt_val]
    yright_child = node_.y[node_.X[node_.feature] > splt_val]

    node_.children += [self.dfs(node(node_, None, Xleft_child, yleft_child,node_.depth+1))]
    node_.children += [self.dfs(node(node_, None, Xright_child, yright_child,node_.depth+1))]

    return node_

  def train(self):
    self.root = self.dfs(node(None, None, self.X, self.y, 0))

  def classify(self, node_, x):
    if (self.leaf_node(node_)):
      return int(node_.y.mode()[0])
    
    if (x[node_.feature] <= node_.pVal):
      return self.classify(node_.children[0], x)
    else:
      return self.classify(node_.children[1], x)

  def test(self, X_test, y_test):
    correct_classifications = 0

    for i in range(len(X_test.index)):
      x = X_test.iloc[i]
      y = y_test.iloc[i]
      if (y == self.classify(self.root, x)):
        correct_classifications+=1

      return correct_classifications/len(X_test.index) *100.0

dct = DecisionTree(X_train, y_train, 10)
dct.train()

X_test.iloc[len(X_test.index) - 1]

dct.test(X_test, y_test) * 100.0